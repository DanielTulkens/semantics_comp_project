{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | N | PropN | Adj NP | NP PP\n",
    "    VP -> V | Vbar NP | Vbar NP PP | Adv VP | VP Adv | VP Conj VP | V AdvP\n",
    "    Vbar -> V\n",
    "    AdvP -> Adv P\n",
    "    PP -> P NP\n",
    "    Det -> 'a' | 'an' | 'the' | 'every'| 'some' | 'any'\n",
    "    P -> 'with' | 'in' | 'on' | 'to' | 'without' | 'from'\n",
    "    Conj -> 'and' | 'or' | 'but'\n",
    "\n",
    "\n",
    "    N -> 'boy' | 'student' | 'girl' | 'class' | 'book' | 'teacher'\n",
    "    PropN -> 'john' | 'mary'\n",
    "    Adj -> 'eager' | 'smart'\n",
    "    V -> 'walks' | 'passed' | 'sees' | 'studies' | 'teaches' | 'saw' \n",
    "    Adv -> 'eagerly' | 'well'\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [ 'John sees Mary', 'A student walks', 'Some girl sees every boy','Every eager student passed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = random.choice(sentences)\n",
    "parser = nltk.parse.BottomUpLeftCornerChartParser(grammar)                            \n",
    "tokens = sentence.lower().split()\n",
    "parse_trees = parser.parse(tokens)\n",
    "if not parse_trees:\n",
    "    print(\"No parse tree found.\")\n",
    "for parse_tree in parse_trees:\n",
    "    print(parse_tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretty_print\n",
    "sentence = random.choice(sentences)\n",
    "parser = nltk.parse.BottomUpLeftCornerChartParser(grammar)                            \n",
    "tokens = sentence.lower().split()\n",
    "parse_trees = parser.parse(tokens)\n",
    "if not parse_trees:\n",
    "    print(\"No parse tree found.\")\n",
    "for parse_tree in parse_trees:\n",
    "    parse_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det some) (N girl))\n",
      "  (VP (Vbar (V sees)) (NP (Det every) (N boy))))\n",
      "(NP (Det some) (N girl))\n",
      "(Det some)\n",
      "(N girl)\n",
      "(VP (Vbar (V sees)) (NP (Det every) (N boy)))\n",
      "(Vbar (V sees))\n",
      "(V sees)\n",
      "(NP (Det every) (N boy))\n",
      "(Det every)\n",
      "(N boy)\n",
      "\n",
      "\n",
      "()\n",
      "(0,)\n",
      "(0, 0)\n",
      "(0, 0, 0)\n",
      "(0, 1)\n",
      "(0, 1, 0)\n",
      "(1,)\n",
      "(1, 0)\n",
      "(1, 0, 0)\n",
      "(1, 0, 0, 0)\n",
      "(1, 1)\n",
      "(1, 1, 0)\n",
      "(1, 1, 0, 0)\n",
      "(1, 1, 1)\n",
      "(1, 1, 1, 0)\n",
      "\n",
      "\n",
      "(0, 0, 0)\n",
      "(0, 1, 0)\n",
      "(1, 0, 0, 0)\n",
      "(1, 1, 0, 0)\n",
      "(1, 1, 1, 0)\n",
      "\n",
      "\n",
      "(0, 0, 0)\n",
      "(0, 0)\n",
      "(0, 1, 0)\n",
      "(0, 1)\n",
      "(0,)\n",
      "(1, 0, 0, 0)\n",
      "(1, 0, 0)\n",
      "(1, 0)\n",
      "(1, 1, 0, 0)\n",
      "(1, 1, 0)\n",
      "(1, 1, 1, 0)\n",
      "(1, 1, 1)\n",
      "(1, 1)\n",
      "(1,)\n",
      "()\n",
      "\n",
      "\n",
      "the verb\n",
      "[(0, 0, 0), (0, 1, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]\n",
      "[(0, 0, 0), (0, 1, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]\n",
      "[(0, 0, 0), (0, 1, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]\n",
      "[(0, 0, 0), (0, 1, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]\n",
      "[(0, 0, 0), (0, 1, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]\n",
      "[(0, 0, 0), (0, 1, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]\n",
      "[(0, 0, 0), (0, 1, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]\n",
      "[(0, 0, 0), (0, 1, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]\n",
      "[(0, 0, 0), (0, 1, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]\n",
      "[(0, 0, 0), (0, 1, 0), (1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]\n"
     ]
    }
   ],
   "source": [
    "#practice iterating\n",
    "for subtree in parse_tree.subtrees(): \n",
    "    print(subtree)\n",
    "print('\\n' )\n",
    "for pos in parse_tree.treepositions(): \n",
    "    print(pos)\n",
    "print('\\n')\n",
    "for pos in parse_tree.treepositions(order=\"leaves\"):\n",
    "    print(pos)\n",
    "print('\\n')\n",
    "for pos in parse_tree.treepositions(order=\"postorder\"):\n",
    "  print(pos)\n",
    "print('\\n')\n",
    "\n",
    "# Define a function that takes a parse tree, and returns the position of the verb (i.e., a tuple).\n",
    "#  If your grammar follows the textbook closely, then the position of the verb will depend on\n",
    "#  whether it is a transitive or intransitive verb (i.e., depending on the presence/absence of a Vbar node).\n",
    "\n",
    "def get_verb_tuple(tree):\n",
    "    for subtree in tree.subtrees():\n",
    "        if  'Vi' or 'Vt' in subtree:\n",
    "            print(tree.treepositions(subtree))\n",
    "        else:\n",
    "            break\n",
    "print('the verb')\n",
    "verb = get_verb_tuple(parse_tree)\n",
    "\n",
    "#Define a function that takes a position in the parse tree (i.e., a tuple like (0, 1, 1)) and returns the \n",
    "# position of the parent node ((0, 1) in this case), or None if it has no parent.\n",
    "\n",
    "def get_parent_node(child_tuple): \n",
    "    if len(child_tuple) >= 1: \n",
    "        parent_node = child_tuple[:-1]\n",
    "        return parent_node\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "#Define a function that takes a parse tree, and returns the position of the subject DP.\n",
    "def get_position_subj (tree):\n",
    "    pass\n",
    "\n",
    "#Define a function that takes a parse tree, and returns the position of the object DP if one exists, and None otherwise\n",
    "def get_position_obj (tree):\n",
    "    if  'Vt' in parse_tree.subtrees()[0]: \n",
    "        obj =  parse_tree.subtres()[1,1,1]\n",
    "        return obj\n",
    "    else: \n",
    "        None \n",
    "\n",
    "sentence = sentences[0]\n",
    "parser = nltk.parse.BottomUpLeftCornerChartParser(grammar)                                  \n",
    "tokens = sentence.lower().split()\n",
    "parse_trees = parser.parse(tokens)\n",
    "if parse_tree in parse_trees:      \n",
    "    print(parse_tree)\n",
    "    print(get_position_obj(parse_tree))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2292214753.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/12/g9jxbl9x6wgd_gz_t2l352pc0000gn/T/ipykernel_31581/2292214753.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    'Every eager student passed': }\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "translations = { \n",
    "    'John sees Mary' : 'SEES(j,m)', \n",
    "    'A student walks': 'some x [WALK(x)]', \n",
    "    'Some girl sees every boy': 'all y some x [(GIRL(x) ∧ BOY(y)) → SEES(x, y))]',\n",
    "    'Every eager student passed': }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {\n",
    "    'P': (\n",
    "        'with',\n",
    "        'in',\n",
    "        'on',\n",
    "        'to',\n",
    "        'without',\n",
    "        'from'\n",
    "    ),\n",
    "    'N': (\n",
    "        'boy',\n",
    "        'student',\n",
    "        'girl',\n",
    "        'class',\n",
    "        'book',\n",
    "        'teacher'\n",
    "    ),\n",
    "    'PropN': (\n",
    "        'john',\n",
    "        'mary'\n",
    "    ),\n",
    "    'Vi': (\n",
    "        'walks',\n",
    "        'passed'\n",
    "    ),\n",
    "    'Vt': (\n",
    "        'sees',\n",
    "        'teaches'\n",
    "    ),\n",
    "    'Adj': (\n",
    "        'eager',\n",
    "        'smart'\n",
    "    ),\n",
    "    'Adv': (\n",
    "        'eagerly',\n",
    "        'well'\n",
    "    ),\n",
    "    'Det': (\n",
    "        'a',\n",
    "        'an',\n",
    "        'the',\n",
    "        'every',\n",
    "        'some',\n",
    "        'any'\n",
    "    ),\n",
    "    'Conj': (\n",
    "        'and',\n",
    "        'or',\n",
    "        'but'\n",
    "    )\n",
    "}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class would create an object storing both the representation of an element, and its typing information\n",
    "class Formalization:\n",
    "    def __init__(self, formula, type_hint, type_=None):\n",
    "        self.formula = formula\n",
    "        self.type = type_hint\n",
    "        self.t = type_\n",
    "        if type(type_hint) == tuple:\n",
    "            self.selected = type_hint[0]\n",
    "            self.returned = type_hint[1]\n",
    "        else:\n",
    "            self.selected = None\n",
    "\n",
    "    def application(self, argument):\n",
    "        if argument.type == self.selected:\n",
    "            resulting_formula = self.formula(argument.formula)\n",
    "            return Formalization(resulting_formula, self.returned)\n",
    "\n",
    "\n",
    "# this dictionary contains functions to generate lexical entries for different parts of speech.\n",
    "formalizations = {\n",
    "    'PropN': lambda name: Formalization(\n",
    "        name,\n",
    "        'e', 'PropN'\n",
    "    ),\n",
    "    'N': lambda noun: Formalization(\n",
    "        lambda x: f'{noun.upper()}({x})',\n",
    "        ('e', 't'), 'N'\n",
    "    ),\n",
    "    'Vi': lambda verb: Formalization(\n",
    "        lambda x: f'{verb.upper()}({x})',\n",
    "        ('e', 't'), 'Vi'\n",
    "    ),\n",
    "    'Vt': lambda verb: Formalization(\n",
    "        lambda y: lambda x: f'{verb.upper()}({x}, {y})',\n",
    "        ('e', ('e', 't')), 'Vt'\n",
    "    ),\n",
    "    'Adj': lambda adjective: Formalization(\n",
    "        lambda P: lambda x: f'{adjective.upper()}({x}) ^ {P(x)}',\n",
    "        (('e', 't'), ('e', 't')), 'Adj'\n",
    "    ),\n",
    "    'Adv': lambda adverb: Formalization(\n",
    "        lambda P: lambda x: f'{adverb.upper()}({P(x)})',\n",
    "        (('e', 't'), ('e', 't')), 'Adv'\n",
    "    ),\n",
    "    'P': lambda preposition: Formalization(\n",
    "        lambda x: lambda y: f'{preposition.upper()}({x}, {y})',\n",
    "        ('e', ('e', 't')), 'P'\n",
    "    ),\n",
    "    'existential': lambda _: Formalization(\n",
    "        lambda P: lambda Q: f'Exists x[{P(\"x\")} ^ {Q(\"x\")}]',\n",
    "        (('e', 't'), (('e', 't'), 't')), 'existential'\n",
    "    ),\n",
    "    'universal': lambda _: Formalization(\n",
    "        lambda P: lambda Q: f'All x[{P(\"x\")} -> {Q(\"x\")}]',\n",
    "        (('e', 't'), (('e', 't'), 't')), 'universal'\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def generate_lexicon(vocab, translations):\n",
    "    lexicon = {}\n",
    "    for part_of_speech, entries in vocab.items():\n",
    "        if part_of_speech in translations.keys():\n",
    "            lexicon.update({part_of_speech: {word: translations[part_of_speech](word) for word in entries}})\n",
    "        elif type(entries) == dict:\n",
    "            lexicon[part_of_speech] = {}\n",
    "            for subtype, words in entries.items():\n",
    "                lexicon[part_of_speech].update({word: translations[subtype](word) for word in words})\n",
    "        else:\n",
    "            lexicon.update({part_of_speech: {word: Formalization(word, 'e') for word in entries}})\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def lexicon_to_terminals(lexicon):\n",
    "    terminal_rules = ''\n",
    "    for part_of_speech in lexicon.keys():\n",
    "        terminals_string = \" | \".join([\"'\" + word + \"'\" for word in lexicon[part_of_speech].keys()])\n",
    "        terminal_rules += (part_of_speech + ' -> ' + terminals_string + '\\n\\t')\n",
    "    return terminal_rules\n",
    "\n",
    "\n",
    "extensional_lexicon = generate_lexicon(vocabulary, formalizations)\n",
    "\n",
    "terminals_entries = lexicon_to_terminals(extensional_lexicon)\n",
    "\n",
    "extensional_grammar = f\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det Nom | PropN\n",
    "    Nom -> N  | Adj Nom | Nom PP\n",
    "    VP -> Vi | Vt  | Vbar NP | Vbar NP PP | Adv VP | VP Adv | VP Conj VP | V AdvP\n",
    "    Vbar -> Vi | Vt\n",
    "    AdvP -> Adv P\n",
    "    PP -> P NP\n",
    "    {terminals_entries}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       S            \n",
      "   ____|____         \n",
      "  |         VP      \n",
      "  |     ____|____    \n",
      "  NP  Vbar       NP \n",
      "  |    |         |   \n",
      "PropN  V       PropN\n",
      "  |    |         |   \n",
      " john sees      mary\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'V'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/12/g9jxbl9x6wgd_gz_t2l352pc0000gn/T/ipykernel_31581/2970119665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mparse_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mparse_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_to_logic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/12/g9jxbl9x6wgd_gz_t2l352pc0000gn/T/ipykernel_31581/2970119665.py\u001b[0m in \u001b[0;36mtranslate_to_logic\u001b[0;34m(tree)\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                         \u001b[0mapplier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformalizations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mapplier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                     \u001b[0mroot_cause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mprev_was_leave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'V'"
     ]
    }
   ],
   "source": [
    "existentials = ['a', 'some', 'an']\n",
    "universal = ['every']\n",
    "def translate_to_logic(tree = nltk.Tree):\n",
    "    result = {}\n",
    "    prev_was_leave = False\n",
    "    leaf = ''\n",
    "    root_cause = None\n",
    "    for node in tree.treepositions(order='postorder'):\n",
    "        if isinstance(tree[node], nltk.Tree):\n",
    "            if len(tree[node]) > 1:\n",
    "                if tree[node].label() == 'S':\n",
    "                    if result[node + (1,)].t == 'existential' or result[node + (1,)].t =='universal': #checks if the vp has an quantifier\n",
    "                        result[node] = result[node + (0,)].application(result[node + (1,)]) #it will apple the r child to the left \n",
    "                    else:\n",
    "                        result[node] = result[node + (1,)].application(result[node + (0,)])\n",
    "                else:\n",
    "                    result[node] = result[node + (0,)].application(result[node + (1,)])\n",
    "            else:\n",
    "                if prev_was_leave: \n",
    "                    if leaf in existentials:\n",
    "                        applier = 'existential'\n",
    "                    elif leaf in universal:\n",
    "                        applier = 'universal'\n",
    "                    else:\n",
    "                        applier = tree[node].label()\n",
    "                    result[node] = formalizations[applier](leaf)\n",
    "                    root_cause = result[node]\n",
    "                    prev_was_leave = False\n",
    "                else:\n",
    "                    result[node] = root_cause\n",
    "        else:\n",
    "            prev_was_leave = True\n",
    "            leaf = tree[node]\n",
    "    return result\n",
    "sentence = sentences[0]\n",
    "parser = nltk.parse.BottomUpLeftCornerChartParser(grammar)                                  \n",
    "tokens = sentence.lower().split()\n",
    "parse_tree = list(parser.parse(tokens))[0]\n",
    "parse_tree.pretty_print()\n",
    "res = translate_to_logic(parse_tree)\n",
    "res[()].formula"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
